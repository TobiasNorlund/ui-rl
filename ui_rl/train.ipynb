{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64445d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/ui-rl/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from uitars import rollout_to_messages\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from generate_rollout_batch import deserialize_rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fd381e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:08<00:00,  1.23s/it]\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"ByteDance-Seed/UI-TARS-1.5-7B\")\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"ByteDance-Seed/UI-TARS-1.5-7B\", \n",
    "    device_map=\"cuda\", \n",
    "    dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44c450d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flash_attention_2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(model.config, \"_attn_implementation\", \"default (eager or sdpa)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "187d970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],   # You may adjust to match the linear/attention modules in Qwen2_5_VL\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b06a66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model.model.model.visual.patch_embed.proj.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac2afe00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2_5_VLForConditionalGeneration(\n",
       "      (model): Qwen2_5_VLModel(\n",
       "        (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
       "          (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
       "            (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "          )\n",
       "          (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
       "          (blocks): ModuleList(\n",
       "            (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
       "              (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "              (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "              (attn): Qwen2_5_VLVisionAttention(\n",
       "                (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "                (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              )\n",
       "              (mlp): Qwen2_5_VLMLP(\n",
       "                (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
       "                (up_proj): Linear(in_features=1280, out_features=3420, bias=True)\n",
       "                (down_proj): Linear(in_features=3420, out_features=1280, bias=True)\n",
       "                (act_fn): SiLUActivation()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (merger): Qwen2_5_VLPatchMerger(\n",
       "            (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "            (mlp): Sequential(\n",
       "              (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "              (1): GELU(approximate='none')\n",
       "              (2): Linear(in_features=5120, out_features=3584, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (language_model): Qwen2_5_VLTextModel(\n",
       "          (embed_tokens): Embedding(152064, 3584)\n",
       "          (layers): ModuleList(\n",
       "            (0-27): 28 x Qwen2_5_VLDecoderLayer(\n",
       "              (self_attn): Qwen2_5_VLAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=3584, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=3584, out_features=512, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=3584, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=512, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "                (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): Qwen2MLP(\n",
       "                (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "                (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "                (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "                (act_fn): SiLUActivation()\n",
       "              )\n",
       "              (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "              (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "            )\n",
       "          )\n",
       "          (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "          (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "        )\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d51727e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = deserialize_rollout(\"../runs/20251031_204956/rollout_000/rollout.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbeccc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = rollout_to_messages(rollout)\n",
    "for message in messages:\n",
    "    if type(message[\"content\"]) != list:\n",
    "        message[\"content\"] = [{\"type\": \"text\", \"text\": message[\"content\"]}]\n",
    "    for block in message[\"content\"]:\n",
    "        if block[\"type\"] == \"image_url\":\n",
    "            block[\"type\"] = \"image\"\n",
    "            block[\"image\"] = block[\"image_url\"][\"url\"]\n",
    "            del block[\"image_url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f8a89de",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor.apply_chat_template(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b298e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor.apply_chat_template(\n",
    "\tmessages,\n",
    "\t#add_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8b75fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,   8948,    198,  ..., 151653, 151645,    198]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0'), 'pixel_values': tensor([[1.4340, 1.4340, 1.4340,  ..., 2.1459, 2.1459, 2.1459],\n",
       "        [1.4340, 1.4340, 1.4340,  ..., 2.1459, 2.1459, 2.1459],\n",
       "        [1.4340, 1.4340, 1.4340,  ..., 2.1459, 2.1459, 2.1459],\n",
       "        ...,\n",
       "        [1.7114, 1.7114, 1.7114,  ..., 2.0464, 2.0464, 2.0464],\n",
       "        [1.7114, 1.7114, 1.7114,  ..., 2.0464, 2.0464, 2.0464],\n",
       "        [1.7114, 1.7114, 1.7114,  ..., 2.0464, 2.0464, 2.0464]],\n",
       "       device='cuda:0'), 'image_grid_thw': tensor([[ 1, 58, 92],\n",
       "        [ 1, 58, 92],\n",
       "        [ 1, 58, 92],\n",
       "        [ 1, 58, 92],\n",
       "        [ 1, 58, 92],\n",
       "        [ 1, 58, 92],\n",
       "        [ 1, 58, 92],\n",
       "        [ 1, 58, 92],\n",
       "        [ 1, 58, 92],\n",
       "        [ 1, 58, 92]], device='cuda:0'), 'labels': tensor([[151644,   8948,    198,  ..., 151653, 151645,    198]],\n",
       "       device='cuda:0')}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"labels\"] = inputs.input_ids\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2185d688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14522])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fc8f50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/ui-rl/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::mul         0.36%       9.798ms         3.39%      92.147ms     134.718us     187.146ms         9.01%     208.133ms     304.287us     215.94 KB     215.94 KB     111.63 GB     111.63 GB           684  \n",
      "                                    aten::empty_strided         0.20%       5.541ms         0.28%       7.596ms      11.073us       0.000us         0.00%       0.000us       0.000us     233.53 KB     233.53 KB     108.51 GB     108.51 GB           686  \n",
      "                                           aten::linear         0.11%       2.968ms        13.63%     371.057ms     787.808us       0.000us         0.00%        2.198s       4.667ms           0 B           0 B      90.33 GB           0 B           471  \n",
      "                                               aten::to         0.04%       1.166ms         1.74%      47.447ms      64.031us       0.000us         0.00%     231.422ms     312.310us     233.53 KB           0 B      88.15 GB           0 B           741  \n",
      "                                         aten::_to_copy         0.16%       4.361ms         1.70%      46.281ms      90.042us       0.000us         0.00%     231.422ms     450.237us     233.53 KB           0 B      88.15 GB           0 B           514  \n",
      "                                            aten::addmm         3.93%     107.103ms         9.25%     251.746ms       1.023ms     464.703ms        22.36%        1.330s       5.406ms           0 B           0 B      45.84 GB      45.84 GB           246  \n",
      "                                           aten::matmul         0.09%       2.466ms         6.45%     175.500ms     776.549us       0.000us         0.00%     868.562ms       3.843ms           0 B           0 B      44.50 GB           0 B           226  \n",
      "                                               aten::mm         0.32%       8.634ms         3.91%     106.321ms     472.537us     738.995ms        35.56%     868.524ms       3.860ms           0 B           0 B      44.49 GB      44.49 GB           225  \n",
      "                                              aten::add         0.26%       7.140ms         0.62%      16.814ms      33.493us      75.374ms         3.63%      83.657ms     166.647us     764.82 KB     764.82 KB      39.18 GB      39.18 GB           502  \n",
      "                                              aten::pow         0.09%       2.528ms         1.32%      35.971ms     294.843us      34.154ms         1.64%      37.001ms     303.290us           0 B           0 B      27.59 GB      27.59 GB           122  \n",
      "                                             aten::silu         0.04%       1.134ms         0.28%       7.679ms     127.975us      31.352ms         1.51%      32.609ms     543.481us           0 B           0 B      25.23 GB      25.23 GB            60  \n",
      "                                       aten::empty_like         0.03%     910.245us         0.10%       2.641ms      11.584us       0.000us         0.00%       0.000us       0.000us       1.13 MB           0 B      20.55 GB           0 B           228  \n",
      "                                              aten::cat         0.20%       5.488ms         0.43%      11.742ms      58.129us      45.436ms         2.19%      50.206ms     248.545us       2.37 MB       2.37 MB      19.71 GB      19.71 GB           202  \n",
      "                                   aten::native_dropout         0.04%       1.179ms         2.35%      63.970ms       1.142ms      16.159ms         0.78%      21.341ms     381.094us           0 B           0 B      13.57 GB           0 B            56  \n",
      "                                          aten::dropout         0.01%     371.777us         2.36%      64.341ms       1.149ms       0.000us         0.00%      21.341ms     381.094us           0 B           0 B      10.86 GB      -2.71 GB            56  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.722s\n",
      "Self CUDA time total: 2.078s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "# 1. Enable memory profiling and capture the forward pass\n",
    "with profile(activities=[\n",
    "        ProfilerActivity.CPU, \n",
    "        ProfilerActivity.CUDA], \n",
    "    profile_memory=True, \n",
    "    record_shapes=True) as prof:\n",
    "    \n",
    "    # Run your problematic forward pass here\n",
    "    # Ensure your model and input tensors are on the GPU\n",
    "    #with torch.no_grad():\n",
    "    output = model(**inputs)\n",
    "\n",
    "# 2. Print a table summarizing the CUDA memory events\n",
    "print(prof.key_averages().table(sort_by=\"cuda_memory_usage\", row_limit=15))\n",
    "\n",
    "# 3. Optional: Export for a visual timeline (see Step 2)\n",
    "# prof.export_chrome_trace(\"trace.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7d49211",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "078b4f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,046,272 || all params: 8,297,212,928 || trainable%: 0.0608\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "402c8fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Listing Live PyTorch Tensors on GPU 0 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/ui-rl/.venv/lib/python3.12/site-packages/torch/__init__.py:1125: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(obj, torch.Tensor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tensors Found: 920\n",
      "-----------------------------------------------------------------------\n",
      "Size (MB)    Shape                          Dtype        Grad \n",
      "-----------------------------------------------------------------------\n",
      "4211.947    (1, 14522, 152064)             bfloat16     True \n",
      "1039.500    (152064, 3584)                 bfloat16     False\n",
      "1039.500    (152064, 3584)                 bfloat16     False\n",
      "239.377     (53360, 1176)                  float32      False\n",
      "198.543     (1, 14522, 3584)               float32      True \n",
      "143.626     (32016, 1176)                  float32      False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (18944, 3584)                  bfloat16     False\n",
      "129.500     (3584, 18944)                  bfloat16     False\n",
      "99.271      (1, 14522, 3584)               bfloat16     True \n",
      "99.271      (1, 14522, 3584)               bfloat16     True \n",
      "99.271      (1, 14522, 3584)               bfloat16     True \n",
      "99.271      (1, 14522, 3584)               bfloat16     True \n",
      "99.271      (1, 14522, 3584)               bfloat16     True \n",
      "99.271      (1, 14522, 3584)               bfloat16     True \n",
      "99.271      (1, 14522, 3584)               bfloat16     True \n",
      "99.271      (1, 14522, 3584)               bfloat16     True \n",
      "99.271      (1, 14522, 3584)               bfloat16     True \n",
      "99.271      (1, 14522, 3584)               bfloat16     True \n",
      "-----------------------------------------------------------------------\n",
      "Total size of all tensors: 23579.345 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def get_gpu_tensors_sorted_by_size(device_id=0):\n",
    "    \"\"\"\n",
    "    Retrieves all PyTorch tensors currently in GPU memory, calculates their size,\n",
    "    and returns a sorted list of their properties.\n",
    "    \"\"\"\n",
    "    print(f\"--- Listing Live PyTorch Tensors on GPU {device_id} ---\")\n",
    "    \n",
    "    tensor_info = []\n",
    "    \n",
    "    # 1. Iterate through all objects tracked by the Python garbage collector\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) and obj.is_cuda and obj.get_device() == device_id:\n",
    "                # Calculate size in bytes: element_size * number_of_elements\n",
    "                size_in_bytes = obj.element_size() * obj.nelement()\n",
    "                \n",
    "                tensor_info.append({\n",
    "                    'size_MB': size_in_bytes / (1024 * 1024),\n",
    "                    'shape': tuple(obj.size()),\n",
    "                    'dtype': obj.dtype,\n",
    "                    'requires_grad': obj.requires_grad,\n",
    "                    # Optional: Add traceback/origin for more advanced debugging if needed\n",
    "                })\n",
    "        except Exception:\n",
    "            # Safely skip objects that raise an error during property check\n",
    "            continue\n",
    "\n",
    "    # 2. Sort the list by size in descending order\n",
    "    tensor_info.sort(key=lambda x: x['size_MB'], reverse=True)\n",
    "    \n",
    "    # 3. Print the sorted list\n",
    "    print(f\"Total Tensors Found: {len(tensor_info)}\")\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    print(f\"{'Size (MB)':<12} {'Shape':<30} {'Dtype':<12} {'Grad':<5}\")\n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "    for info in tensor_info[:100]: # Only list the top 20 largest tensors\n",
    "        size_str = f\"{info['size_MB']:<11.3f}\"\n",
    "        shape_str = f\"{str(info['shape']):<30}\"\n",
    "        dtype_str = f\"{str(info['dtype']).split('.')[-1]:<12}\"\n",
    "        grad_str = f\"{str(info['requires_grad']):<5}\"\n",
    "        print(f\"{size_str} {shape_str} {dtype_str} {grad_str}\")\n",
    "    \n",
    "    print(\"-----------------------------------------------------------------------\")\n",
    "\n",
    "    # Calculate total size in MB of all tensors\n",
    "    total_size_mb = sum(info['size_MB'] for info in tensor_info)\n",
    "    print(f\"Total size of all tensors: {total_size_mb:.3f} MB\")\n",
    "\n",
    "\n",
    "    return tensor_info\n",
    "\n",
    "# Example Usage:\n",
    "# Run this function at the point of your crash (or near it)\n",
    "_ = get_gpu_tensors_sorted_by_size(device_id=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
